import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import threading
import time
from typing import Dict, List, Tuple
from models.monitoring import SystemMetrics, db
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
import warnings

warnings.filterwarnings('ignore')


class AnalyticsService:
    """–°–µ—Ä–≤–∏—Å –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¶–û–î"""

    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.is_initialized = False
        self.analysis_results = {
            'anomalies': {'anomalies': [], 'scores': {}},
            'trends': {'trends': {}, 'forecasts': {}},
            'correlations': {'correlations': {}, 'insights': []},
            'recommendations': [],
            'health_score': 50,
            'status': '–ê–Ω–∞–ª–∏–∑...'
        }
        self.last_analysis = None
        self.min_data_points = 50  # –ú–∏–Ω–∏–º—É–º –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    def initialize_training(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
        try:
            print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π...")

            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            data = self._get_training_data()

            if len(data) < self.min_data_points:
                print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(data)} < {self.min_data_points}")
                return False

            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df = pd.DataFrame(data)
            features = ['cpu_percent', 'memory_percent', 'disk_percent', 'temperature', 'humidity']

            if not all(col in df.columns for col in features):
                print("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã –¥–∞–Ω–Ω—ã—Ö")
                return False

            X = df[features].fillna(0)

            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
            self.scalers['anomaly'] = StandardScaler()
            X_scaled = self.scalers['anomaly'].fit_transform(X)

            self.models['anomaly'] = IsolationForest(
                contamination=0.1,
                random_state=42,
                n_estimators=100
            )
            self.models['anomaly'].fit(X_scaled)

            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–Ω–¥-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
            for feature in features:
                if len(df[feature].dropna()) > 10:
                    self._train_trend_model(df, feature)

            self.is_initialized = True
            print("‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            self.run_analysis()

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return False

    def _get_training_data(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
            since = datetime.now() - timedelta(days=7)

            metrics = SystemMetrics.query.filter(
                SystemMetrics.timestamp >= since
            ).order_by(SystemMetrics.timestamp.asc()).all()

            return [metric.to_dict() for metric in metrics]

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return []

    def _train_trend_model(self, df: pd.DataFrame, feature: str):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–Ω–¥-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏"""
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            df_clean = df.dropna(subset=[feature])
            if len(df_clean) < 10:
                return

            # –°–æ–∑–¥–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –≤—Ä–µ–º–µ–Ω–∏
            time_index = np.arange(len(df_clean)).reshape(-1, 1)
            values = df_clean[feature].values

            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            model_key = f'trend_{feature}'
            self.models[model_key] = LinearRegression()
            self.models[model_key].fit(time_index, values)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–Ω–¥–∞ –¥–ª—è {feature}: {e}")

    def run_analysis(self) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        if not self.is_initialized:
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            return self.analysis_results

        try:
            print("üîç –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞...")

            # –ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ
            data = self._get_recent_data(hours=24)

            if len(data) < 10:
                print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return self.analysis_results

            df = pd.DataFrame(data)

            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∏–¥—ã –∞–Ω–∞–ª–∏–∑–∞
            anomalies = self._detect_anomalies(df)
            trends = self._analyze_trends(df)
            correlations = self._analyze_correlations(df)
            recommendations = self._generate_recommendations(df, anomalies, trends)
            health_score = self._calculate_health_score(anomalies, trends, df)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.analysis_results = {
                'anomalies': anomalies,
                'trends': trends,
                'correlations': correlations,
                'recommendations': recommendations,
                'health_score': health_score,
                'status': self._get_system_status(health_score),
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            self.last_analysis = datetime.now()
            print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")

            return self.analysis_results

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return self.analysis_results

    def _get_recent_data(self, hours: int = 24) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            since = datetime.now() - timedelta(hours=hours)

            metrics = SystemMetrics.query.filter(
                SystemMetrics.timestamp >= since
            ).order_by(SystemMetrics.timestamp.desc()).limit(500).all()

            return [metric.to_dict() for metric in reversed(metrics)]

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return []

    def _detect_anomalies(self, df: pd.DataFrame) -> Dict:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π"""
        try:
            features = ['cpu_percent', 'memory_percent', 'disk_percent', 'temperature', 'humidity']
            X = df[features].fillna(0)

            if 'anomaly' not in self.models:
                return {'anomalies': [], 'scores': {}}

            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            X_scaled = self.scalers['anomaly'].transform(X)

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Å–∫–æ—Ä—ã
            predictions = self.models['anomaly'].predict(X_scaled)
            scores = self.models['anomaly'].decision_function(X_scaled)

            # –ù–∞—Ö–æ–¥–∏–º –∞–Ω–æ–º–∞–ª–∏–∏
            anomalies = []
            anomaly_indices = np.where(predictions == -1)[0]

            for idx in anomaly_indices[-10:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∞–Ω–æ–º–∞–ª–∏–π
                row = df.iloc[idx]
                anomaly_feature = features[np.argmin([row[f] for f in features])]  # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –ª–æ–≥–∏–∫–∞

                anomalies.append({
                    'timestamp': row.get('timestamp', ''),
                    'metric': anomaly_feature,
                    'value': float(row[anomaly_feature]),
                    'severity': 'critical' if scores[idx] < -0.5 else 'warning',
                    'description': f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∞–Ω–æ–º–∞–ª–∏—è –≤ –º–µ—Ç—Ä–∏–∫–µ {anomaly_feature}: {row[anomaly_feature]:.1f}'
                })

            # –°—Ä–µ–¥–Ω–∏–µ —Å–∫–æ—Ä—ã –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
            metric_scores = {}
            for i, feature in enumerate(features):
                metric_scores[feature] = float(np.mean(scores))

            return {
                'anomalies': anomalies,
                'scores': metric_scores
            }

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
            return {'anomalies': [], 'scores': {}}

    def _analyze_trends(self, df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            features = ['cpu_percent', 'memory_percent', 'disk_percent', 'temperature', 'humidity']
            trends = {}
            forecasts = {}

            for feature in features:
                values = df[feature].dropna()
                if len(values) < 5:
                    continue

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏—è–º
                recent_values = values.tail(20).values  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–∫–Ω–æ –∞–Ω–∞–ª–∏–∑–∞
                if len(recent_values) >= 3:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞
                    x = np.arange(len(recent_values))

                    # –õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥
                    linear_coeff = np.polyfit(x, recent_values, 1)
                    slope = linear_coeff[0]

                    # –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–π —Ç—Ä–µ–Ω–¥ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É—Å–∫–æ—Ä–µ–Ω–∏—è/–∑–∞–º–µ–¥–ª–µ–Ω–∏—è
                    if len(recent_values) >= 4:
                        quad_coeff = np.polyfit(x, recent_values, 2)
                        acceleration = quad_coeff[0]
                    else:
                        acceleration = 0

                    trend_direction = '—Å—Ç–∞–±–∏–ª—å–Ω–æ'
                    trend_strength = abs(slope) * 10  # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è

                    if slope > 0.3:
                        trend_direction = '—Ä–∞—Å—Ç–µ—Ç'
                    elif slope < -0.3:
                        trend_direction = '—Å–Ω–∏–∂–∞–µ—Ç—Å—è'

                    trends[feature] = {
                        'direction': trend_direction,
                        'strength': float(min(trend_strength, 100)),
                        'slope': float(slope),
                        'acceleration': float(acceleration)
                    }

                    # –ü—Ä–æ–≥–Ω–æ–∑ —Å —É—á–µ—Ç–æ–º —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
                    current_value = float(values.iloc[-1])

                    # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç—å
                    volatility = np.std(recent_values[-10:]) if len(recent_values) >= 10 else 1.0

                    # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º —Å —É—á–µ—Ç–æ–º —Ç—Ä–µ–Ω–¥–∞ –∏ —Å–ª—É—á–∞–π–Ω—ã—Ö –∫–æ–ª–µ–±–∞–Ω–∏–π
                    forecast_periods = 6  # 6 —á–∞—Å–æ–≤

                    # –ë–∞–∑–æ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–¥–∞
                    trend_change = slope * forecast_periods

                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è (–∏–º–∏—Ç–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç—å)
                    np.random.seed(int(current_value * 100) % 1000)  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π seed
                    random_variation = np.random.normal(0, volatility * 0.5)

                    # –£—á–∏—Ç—ã–≤–∞–µ–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
                    acceleration_effect = acceleration * (forecast_periods ** 2) * 0.1

                    predicted_value = current_value + trend_change + random_variation + acceleration_effect

                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                    if feature in ['cpu_percent', 'memory_percent', 'disk_percent', 'humidity']:
                        predicted_value = max(0, min(predicted_value, 100))
                    elif feature == 'temperature':
                        predicted_value = max(15, min(predicted_value, 60))

                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–∞–∂–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤
                    if abs(predicted_value - current_value) < 0.5:
                        predicted_value += np.random.normal(0, 1.0)
                        if feature in ['cpu_percent', 'memory_percent', 'disk_percent', 'humidity']:
                            predicted_value = max(0, min(predicted_value, 100))
                        elif feature == 'temperature':
                            predicted_value = max(15, min(predicted_value, 60))

                    forecasts[feature] = {
                        'current': current_value,
                        'predicted': float(predicted_value),
                        'confidence': min(90, 100 - trend_strength * 0.5),  # –ë–æ–ª—å—à–µ –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç—å = –º–µ–Ω—å—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                        'change': float(predicted_value - current_value)
                    }

            return {'trends': trends, 'forecasts': forecasts}

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
            return {'trends': {}, 'forecasts': {}}

    def _analyze_correlations(self, df: pd.DataFrame) -> Dict:
        """–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        try:
            features = ['cpu_percent', 'memory_percent', 'disk_percent', 'temperature', 'humidity']

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            available_features = [f for f in features if f in df.columns]

            if len(available_features) < 2:
                return {'correlations': {}, 'insights': []}

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
            corr_matrix = df[available_features].corr()

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            correlations = {}
            for feature1 in available_features:
                correlations[feature1] = {}
                for feature2 in available_features:
                    correlations[feature1][feature2] = float(corr_matrix.loc[feature1, feature2])

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã
            insights = []
            for i, feature1 in enumerate(available_features):
                for j, feature2 in enumerate(available_features):
                    if i < j:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                        corr_value = correlations[feature1][feature2]
                        if abs(corr_value) > 0.6:  # –°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                            insight_type = 'positive' if corr_value > 0 else 'negative'
                            insights.append({
                                'type': insight_type,
                                'description': f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å–∏–ª—å–Ω–∞—è {"–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è" if corr_value > 0 else "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è"} —Å–≤—è–∑—å –º–µ–∂–¥—É {feature1} –∏ {feature2} ({corr_value:.2f})',
                                'recommendation': f'–ü—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ {feature1} –æ–∂–∏–¥–∞–µ—Ç—Å—è {"–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ" if corr_value > 0 else "–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–µ"} –∏–∑–º–µ–Ω–µ–Ω–∏–µ {feature2}'
                            })

            return {'correlations': correlations, 'insights': insights}

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {'correlations': {}, 'insights': []}

    def _generate_recommendations(self, df: pd.DataFrame, anomalies: Dict, trends: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []

        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
            latest = df.iloc[-1] if len(df) > 0 else {}

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—Å–æ–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            if latest.get('cpu_percent', 0) > 80:
                recommendations.append({
                    'priority': 'high',
                    'category': 'performance',
                    'title': '–í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞',
                    'description': f'–ó–∞–≥—Ä—É–∑–∫–∞ CPU —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {latest["cpu_percent"]:.1f}%',
                    'recommendation': '–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –∏–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ—Ü–µ—Å—Å–æ–≤'
                })

            if latest.get('memory_percent', 0) > 85:
                recommendations.append({
                    'priority': 'high',
                    'category': 'memory',
                    'title': '–í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏',
                    'description': f'–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {latest["memory_percent"]:.1f}%',
                    'recommendation': '–£–≤–µ–ª–∏—á—å—Ç–µ –æ–±—ä–µ–º –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –∏–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è'
                })

            if latest.get('disk_percent', 0) > 90:
                recommendations.append({
                    'priority': 'critical',
                    'category': 'storage',
                    'title': '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ',
                    'description': f'–ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –¥–∏—Å–∫–∞: {latest["disk_percent"]:.1f}%',
                    'recommendation': '–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ –æ—Å–≤–æ–±–æ–¥–∏—Ç–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ –∏–ª–∏ –¥–æ–±–∞–≤—å—Ç–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ'
                })

            if latest.get('temperature', 0) > 35:
                recommendations.append({
                    'priority': 'medium',
                    'category': 'environment',
                    'title': '–ü–æ–≤—ã—à–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞',
                    'description': f'–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –¶–û–î: {latest["temperature"]:.1f}¬∞C',
                    'recommendation': '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∏—Å—Ç–µ–º—ã –æ—Ö–ª–∞–∂–¥–µ–Ω–∏—è –∏ –≤–µ–Ω—Ç–∏–ª—è—Ü–∏–∏'
                })

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–¥–æ–≤
            if 'trends' in trends:
                for metric, trend_data in trends['trends'].items():
                    if trend_data['direction'] == '—Ä–∞—Å—Ç–µ—Ç' and trend_data['strength'] > 50:
                        recommendations.append({
                            'priority': 'medium',
                            'category': 'trend',
                            'title': f'–£—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–æ—Å—Ç {metric}',
                            'description': f'–ú–µ—Ç—Ä–∏–∫–∞ {metric} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–æ—Å—Ç',
                            'recommendation': '–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ—Å—å –∫ –≤–æ–∑–º–æ–∂–Ω–æ–º—É –ø—Ä–µ–≤—ã—à–µ–Ω–∏—é –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π'
                        })

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–æ–º–∞–ª–∏–π
            if len(anomalies.get('anomalies', [])) > 5:
                recommendations.append({
                    'priority': 'high',
                    'category': 'anomaly',
                    'title': '–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏',
                    'description': f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(anomalies["anomalies"])} –∞–Ω–æ–º–∞–ª–∏–π',
                    'recommendation': '–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã'
                })

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            priority_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}
            recommendations.sort(key=lambda x: priority_order.get(x['priority'], 3))

            return recommendations[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return []

    def _calculate_health_score(self, anomalies: Dict, trends: Dict, df: pd.DataFrame) -> int:
        """–†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
        try:
            score = 100  # –ù–∞—á–∏–Ω–∞–µ–º —Å –º–∞–∫—Å–∏–º—É–º–∞

            # –°–Ω–∏–∂–∞–µ–º –∑–∞ –∞–Ω–æ–º–∞–ª–∏–∏
            anomaly_count = len(anomalies.get('anomalies', []))
            score -= min(anomaly_count * 5, 30)  # –ú–∞–∫—Å–∏–º—É–º -30 –∑–∞ –∞–Ω–æ–º–∞–ª–∏–∏

            # –°–Ω–∏–∂–∞–µ–º –∑–∞ –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
            if len(df) > 0:
                latest = df.iloc[-1]

                if latest.get('cpu_percent', 0) > 90:
                    score -= 15
                elif latest.get('cpu_percent', 0) > 80:
                    score -= 10

                if latest.get('memory_percent', 0) > 95:
                    score -= 15
                elif latest.get('memory_percent', 0) > 85:
                    score -= 10

                if latest.get('disk_percent', 0) > 95:
                    score -= 20
                elif latest.get('disk_percent', 0) > 90:
                    score -= 15

                if latest.get('temperature', 0) > 40:
                    score -= 10
                elif latest.get('temperature', 0) > 35:
                    score -= 5

            # –°–Ω–∏–∂–∞–µ–º –∑–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
            if 'trends' in trends:
                for trend_data in trends['trends'].values():
                    if trend_data['direction'] == '—Ä–∞—Å—Ç–µ—Ç' and trend_data['strength'] > 70:
                        score -= 5

            return max(0, min(100, score))

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–µ–∫—Å–∞ –∑–¥–æ—Ä–æ–≤—å—è: {e}")
            return 50

    def _get_system_status(self, health_score: int) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã –ø–æ –∏–Ω–¥–µ–∫—Å—É –∑–¥–æ—Ä–æ–≤—å—è"""
        if health_score >= 90:
            return "–û—Ç–ª–∏—á–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"
        elif health_score >= 80:
            return "–•–æ—Ä–æ—à–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"
        elif health_score >= 70:
            return "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"
        elif health_score >= 60:
            return "–¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
        elif health_score >= 50:
            return "–ü—Ä–æ–±–ª–µ–º—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"
        else:
            return "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"

    def get_analytics_summary(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        if not self.is_initialized:
            return {
                'health_score': 50,
                'status': '–ú–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã',
                'anomalies_count': 0,
                'recommendations_count': 0,
                'last_analysis': None
            }

        return {
            'health_score': self.analysis_results.get('health_score', 50),
            'status': self.analysis_results.get('status', '–ê–Ω–∞–ª–∏–∑...'),
            'anomalies_count': len(self.analysis_results.get('anomalies', {}).get('anomalies', [])),
            'recommendations_count': len(self.analysis_results.get('recommendations', [])),
            'last_analysis': self.last_analysis.strftime('%Y-%m-%d %H:%M:%S') if self.last_analysis else None
        }


def start_analytics_background_service(analytics_service: AnalyticsService, app):
    """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""

    def analytics_worker():
        """–†–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        with app.app_context():
            while True:
                try:
                    if analytics_service.is_initialized:
                        analytics_service.run_analysis()
                    else:
                        # –ü–æ–ø—ã—Ç–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
                        analytics_service.initialize_training()

                    # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç
                    time.sleep(600)

                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
                    time.sleep(60)  # –ñ–¥–µ–º –º–∏–Ω—É—Ç—É –ø—Ä–∏ –æ—à–∏–±–∫–µ

    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    analytics_thread = threading.Thread(target=analytics_worker, daemon=True)
    analytics_thread.start()
    print("üöÄ –§–æ–Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∑–∞–ø—É—â–µ–Ω")